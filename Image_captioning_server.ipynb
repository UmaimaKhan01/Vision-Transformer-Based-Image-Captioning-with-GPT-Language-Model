{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23e81911",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'litserve'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m urlparse  \u001b[38;5;66;03m# For URL validation\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m  \u001b[38;5;66;03m# For file path operations\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlitserve\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mls\u001b[39;00m  \u001b[38;5;66;03m# LitServe library for API server\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Function to check if the input is a valid URL\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_valid_url\u001b[39m(url):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'litserve'"
     ]
    }
   ],
   "source": [
    "# This script sets up an image captioning server using a pre-trained Vision Transformer model.\n",
    "# The server generates captions for images provided via URL or file path.\n",
    "\n",
    "import requests  # For making HTTP requests\n",
    "import torch  # For using PyTorch capabilities\n",
    "from PIL import Image  # To handle image loading\n",
    "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, GPT2TokenizerFast\n",
    "from urllib.parse import urlparse  # For URL validation\n",
    "import os  # For file path operations\n",
    "import litserve as ls  # LitServe library for API server\n",
    "\n",
    "# Function to check if the input is a valid URL\n",
    "def is_valid_url(url):\n",
    "    try:\n",
    "        result = urlparse(url)\n",
    "        return all([result.scheme, result.netloc, result.path])\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "# Function to load an image from a URL or local file path\n",
    "def get_image(image_source):\n",
    "    if is_valid_url(image_source):\n",
    "        return Image.open(requests.get(image_source, stream=True).raw)\n",
    "    elif os.path.isfile(image_source):\n",
    "        return Image.open(image_source)\n",
    "    raise ValueError(\"Invalid image path or URL.\")\n",
    "\n",
    "# Define the image captioning API class\n",
    "class CaptioningAPI(ls.LitAPI):\n",
    "    def initialize(self, compute_device):\n",
    "        # Determine the computation device (GPU if available, else CPU)\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        # Load the pre-trained Vision-Encoder-Decoder model\n",
    "        model_name = \"nlpconnect/vit-gpt2-image-captioning\"\n",
    "        self.caption_model = VisionEncoderDecoderModel.from_pretrained(model_name).to(self.device)\n",
    "\n",
    "        # Load the tokenizer and image processor\n",
    "        self.caption_tokenizer = GPT2TokenizerFast.from_pretrained(model_name)\n",
    "        self.image_processor = ViTImageProcessor.from_pretrained(model_name)\n",
    "\n",
    "    # Decode the client request to extract image path\n",
    "    def parse_request(self, client_request):\n",
    "        return client_request.get(\"image_path\", \"\")\n",
    "\n",
    "    # Predict caption for the input image\n",
    "    def generate_caption(self, image_source):\n",
    "        # Load the image\n",
    "        image = get_image(image_source)\n",
    "\n",
    "        # Preprocess the image\n",
    "        processed_image = self.image_processor(image, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "        # Generate caption\n",
    "        outputs = self.caption_model.generate(**processed_image)\n",
    "        caption = self.caption_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        return caption\n",
    "\n",
    "    # Encode the server response to return the generated caption\n",
    "    def format_response(self, generated_caption):\n",
    "        return {\"generated_caption\": generated_caption}\n",
    "\n",
    "# Main entry point to run the server\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the API and server\n",
    "    api_instance = CaptioningAPI()\n",
    "    caption_server = ls.LitServer(api_instance, accelerator=\"auto\", devices=1, workers_per_device=1)\n",
    "\n",
    "    # Run the server on the specified port\n",
    "    caption_server.run(port=8000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efc2740",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
