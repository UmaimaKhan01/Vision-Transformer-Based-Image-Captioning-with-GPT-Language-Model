{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d61181f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from transformers import GPT2TokenizerFast\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Load GPT-2 tokenizer\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token to end-of-sequence token\n",
    "\n",
    "class ImageCaptionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class to load image-caption pairs.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_frame, image_directory, transform):\n",
    "        self.data_frame = data_frame\n",
    "        self.image_directory = image_directory\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load and transform image\n",
    "        sample = self.data_frame.iloc[idx]\n",
    "        image_path = os.path.join(self.image_directory, sample['image'])\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        \n",
    "        # Process caption with end-of-text token\n",
    "        caption = f\"{sample['caption']}<|endoftext|>\"\n",
    "        input_ids = torch.tensor(tokenizer.encode(caption, truncation=True))\n",
    "        labels = input_ids.clone()\n",
    "        labels[:-1] = input_ids[1:]\n",
    "        labels[-1] = -100  # Set the last token as ignore index for loss computation\n",
    "        return image, input_ids, labels\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function for padding sequences in batch.\n",
    "    \"\"\"\n",
    "    images, input_ids, labels = zip(*batch)\n",
    "    images = torch.stack(images, dim=0)  # Stack images for batch\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "    return images, input_ids, labels\n",
    "\n",
    "# Define data augmentation and normalization transformations\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Resize((224, 224)),  # Resizing for ViT input requirements\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),  # Normalize\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "valid_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd3b0d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
